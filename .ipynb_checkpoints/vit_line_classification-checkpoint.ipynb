{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line-by-Line Classification with Vision Transformer\n",
    "\n",
    "A clean implementation of sequential scanline classification using a Vision Transformer (ViT) architecture.\n",
    "\n",
    "## Approach\n",
    "\n",
    "- **Input**: Partial scan with j scanlines (where j = 1, 2, ..., max_lines)\n",
    "- **Architecture**: Vision Transformer that treats scanline patches as tokens\n",
    "- **Output**: Classification prediction based on lines seen so far\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Patch embedding: Convert scanlines to tokens\n",
    "2. Positional encoding: Encode spatial + temporal position\n",
    "3. Transformer encoder: Self-attention across all patches\n",
    "4. Classification head: Predict class from aggregated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "The dataset should contain images where each image has:\n",
    "- Shape: [num_lines, line_width] (e.g., [128, 128])\n",
    "- Label: Integer class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for line-by-line classification.\n",
    "    \n",
    "    For each image, we create multiple samples by varying the number of visible lines.\n",
    "    This simulates the real-time scanning scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, min_lines=5, max_lines=128, samples_per_image=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Array of shape [N, max_lines, line_width]\n",
    "            labels: Array of shape [N] with class labels\n",
    "            min_lines: Minimum number of lines to use\n",
    "            max_lines: Maximum number of lines (full image)\n",
    "            samples_per_image: How many random cutoffs to generate per image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.min_lines = min_lines\n",
    "        self.max_lines = max_lines\n",
    "        self.samples_per_image = samples_per_image\n",
    "        \n",
    "        # Normalize each scanline independently\n",
    "        self._normalize_scanlines()\n",
    "        \n",
    "    def _normalize_scanlines(self):\n",
    "        \"\"\"Normalize each scanline to zero mean and unit variance.\"\"\"\n",
    "        for i in range(len(self.images)):\n",
    "            for j in range(self.images.shape[1]):\n",
    "                line = self.images[i, j, :]\n",
    "                mean = line.mean()\n",
    "                std = line.std()\n",
    "                if std > 1e-8:\n",
    "                    self.images[i, j, :] = (line - mean) / std\n",
    "                else:\n",
    "                    self.images[i, j, :] = line - mean\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images) * self.samples_per_image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which image and which random cutoff\n",
    "        img_idx = idx // self.samples_per_image\n",
    "        \n",
    "        # Random number of lines between min and max\n",
    "        num_lines = np.random.randint(self.min_lines, self.max_lines + 1)\n",
    "        \n",
    "        # Get partial image\n",
    "        partial_image = self.images[img_idx, :num_lines, :]\n",
    "        label = self.labels[img_idx]\n",
    "        \n",
    "        return {\n",
    "            'image': torch.FloatTensor(partial_image),\n",
    "            'label': torch.LongTensor([label])[0],\n",
    "            'num_lines': num_lines\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dummy_dataset(n_samples=1000, n_classes=4, max_lines=128, line_width=128):\n",
    "    \"\"\"\n",
    "    Create dummy dataset for testing.\n",
    "    \n",
    "    TODO: Replace this with actual data loading function.\n",
    "    \"\"\"\n",
    "    images = np.random.randn(n_samples, max_lines, line_width).astype(np.float32)\n",
    "    labels = np.random.randint(0, n_classes, n_samples)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual data loading\n",
    "train_images, train_labels = create_dummy_dataset(n_samples=800)\n",
    "val_images, val_labels = create_dummy_dataset(n_samples=100)\n",
    "test_images, test_labels = create_dummy_dataset(n_samples=100)\n",
    "\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Unique classes: {np.unique(train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert scanlines into patch embeddings.\n",
    "    \n",
    "    Each scanline is divided into patches, and each patch is linearly projected\n",
    "    to the embedding dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, line_width=128, patch_size=16, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.line_width = line_width\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches_per_line = line_width // patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projection for patches\n",
    "        self.projection = nn.Linear(patch_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, num_lines, line_width]\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: [batch_size, num_lines * num_patches_per_line, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_lines, line_width = x.shape\n",
    "        \n",
    "        # Reshape into patches: [batch, num_lines, num_patches_per_line, patch_size]\n",
    "        x = x.reshape(batch_size, num_lines, self.num_patches_per_line, self.patch_size)\n",
    "        \n",
    "        # Flatten line and patch dimensions: [batch, num_lines * num_patches_per_line, patch_size]\n",
    "        x = x.reshape(batch_size, num_lines * self.num_patches_per_line, self.patch_size)\n",
    "        \n",
    "        # Project to embedding dimension: [batch, num_tokens, embed_dim]\n",
    "        embeddings = self.projection(x)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Add positional information to patch embeddings.\n",
    "    \n",
    "    We need to encode two types of position:\n",
    "    1. Which line (temporal: 0, 1, 2, ... j-1)\n",
    "    2. Which patch within the line (spatial: 0, 1, 2, ... num_patches-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_lines=128, patches_per_line=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_lines = max_lines\n",
    "        self.patches_per_line = patches_per_line\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        max_len = max_lines * patches_per_line\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be moved to GPU)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for line-by-line classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Patch embedding\n",
    "    2. Add CLS token\n",
    "    3. Positional encoding\n",
    "    4. Transformer encoder layers\n",
    "    5. Classification head (using CLS token)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        line_width=128,\n",
    "        patch_size=16,\n",
    "        num_classes=4,\n",
    "        embed_dim=256,\n",
    "        depth=6,\n",
    "        num_heads=8,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1,\n",
    "        max_lines=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patches_per_line = line_width // patch_size\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(line_width, patch_size, embed_dim)\n",
    "        \n",
    "        # CLS token (learnable)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            embed_dim, \n",
    "            max_lines=max_lines, \n",
    "            patches_per_line=self.patches_per_line,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm architecture\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights following ViT paper.\"\"\"\n",
    "        # Initialize CLS token\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, num_lines, line_width]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding: [batch, num_lines, line_width] -> [batch, num_tokens, embed_dim]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Prepend CLS token: [batch, num_tokens + 1, embed_dim]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Take CLS token output\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "        # Normalize and classify\n",
    "        cls_output = self.norm(cls_output)\n",
    "        logits = self.head(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = VisionTransformer(\n",
    "    line_width=128,\n",
    "    patch_size=16,\n",
    "    num_classes=4,\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1,\n",
    "    max_lines=128\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model created with {num_params:,} trainable parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 50, 128).to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    Pads to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    max_lines = max(item['num_lines'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    line_width = batch[0]['image'].shape[1]\n",
    "    \n",
    "    # Create padded tensors\n",
    "    images = torch.zeros(batch_size, max_lines, line_width)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        num_lines = item['num_lines']\n",
    "        images[i, :num_lines] = item['image']\n",
    "        labels[i] = item['label']\n",
    "    \n",
    "    return {'images': images, 'labels': labels}\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LineByLineDataset(train_images, train_labels, min_lines=10, samples_per_image=5)\n",
    "val_dataset = LineByLineDataset(val_images, val_labels, min_lines=10, samples_per_image=5)\n",
    "test_dataset = LineByLineDataset(test_images, test_labels, min_lines=10, samples_per_image=5)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_epochs = 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct / total})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            images = batch['images'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
    "    auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return total_loss / len(loader), accuracy, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_auroc': []\n",
    "}\n",
    "\n",
    "best_val_auroc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_auroc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auroc'].append(val_auroc)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUROC: {val_auroc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_auroc': val_auroc,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"Saved best model with AUROC: {val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Balanced Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUROC\n",
    "axes[2].plot(history['val_auroc'], label='Validation', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUROC')\n",
    "axes[2].set_title('Validation AUROC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Performance vs. Number of Lines\n",
    "\n",
    "Key analysis: How does performance improve as we see more scanlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_num_lines(model, images, labels, device, line_steps=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance as a function of number of scanlines.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        images: Full images [N, max_lines, line_width]\n",
    "        labels: Labels [N]\n",
    "        device: Device to run on\n",
    "        line_steps: List of line counts to evaluate at (default: [10, 20, 30, ..., 128])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics at each line count\n",
    "    \"\"\"\n",
    "    if line_steps is None:\n",
    "        line_steps = list(range(10, 129, 10))  # Every 10 lines\n",
    "    \n",
    "    model.eval()\n",
    "    results = {'num_lines': [], 'accuracy': [], 'auroc': []}\n",
    "    \n",
    "    for num_lines in tqdm(line_steps, desc='Evaluating vs. num lines'):\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        all_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), 32):  # Batch size 32\n",
    "                batch_images = images[i:i+32, :num_lines, :]\n",
    "                batch_labels = labels[i:i+32]\n",
    "                \n",
    "                batch_images = torch.FloatTensor(batch_images).to(device)\n",
    "                \n",
    "                logits = model(batch_images)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_labels.extend(batch_labels)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "        all_preds = np.array(all_preds)\n",
    "        \n",
    "        accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
    "        auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
    "        \n",
    "        results['num_lines'].append(num_lines)\n",
    "        results['accuracy'].append(accuracy)\n",
    "        results['auroc'].append(auroc)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate on test set\n",
    "results = evaluate_vs_num_lines(model, test_images, test_labels, device)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUROC vs. number of lines\n",
    "ax1.plot(results['num_lines'], results['auroc'], marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Number of Scanlines')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax1.set_title('AUROC vs. Number of Scanlines')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Accuracy vs. number of lines\n",
    "ax2.plot(results['num_lines'], results['accuracy'], marker='o', linewidth=2, markersize=6, color='green')\n",
    "ax2.set_xlabel('Number of Scanlines')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('Accuracy vs. Number of Scanlines')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_vs_lines.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for i, num_lines in enumerate(results['num_lines']):\n",
    "    print(f\"Lines: {num_lines:3d} | AUROC: {results['auroc'][i]:.4f} | Accuracy: {results['accuracy'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on full test set\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Class {i}' for i in range(4)],\n",
    "            yticklabels=[f'Class {i}' for i in range(4)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Visualization (Optional)\n",
    "\n",
    "TODO: Extract and visualize attention maps to understand what the model focuses on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify model to return attention weights\n",
    "# TODO: Visualize which patches/lines the model attends to\n",
    "# This helps interpret model decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export\n",
    "\n",
    "Export model for deployment or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model architecture and weights\n",
    "import json\n",
    "\n",
    "model_config = {\n",
    "    'line_width': 128,\n",
    "    'patch_size': 16,\n",
    "    'num_classes': 4,\n",
    "    'embed_dim': 256,\n",
    "    'depth': 6,\n",
    "    'num_heads': 8,\n",
    "    'mlp_ratio': 4,\n",
    "    'dropout': 0.1,\n",
    "    'max_lines': 128\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_config': model_config,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_val_auroc': best_val_auroc\n",
    "}, 'final_model.pt')\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Best validation AUROC: {best_val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a Vision Transformer for line-by-line classification. The key features:\n",
    "\n",
    "1. **Patch-based tokenization**: Each scanline is divided into patches\n",
    "2. **Positional encoding**: Encodes both spatial (within-line) and temporal (which line) information\n",
    "3. **Transformer encoder**: Self-attention across all patches from all visible lines\n",
    "4. **CLS token**: Global representation for classification\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Replace dummy data with actual dataset\n",
    "2. Tune hyperparameters (patch size, model depth, etc.)\n",
    "3. Implement attention visualization\n",
    "4. Compare with baseline methods\n",
    "5. Port to Rust/Burn if needed for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
