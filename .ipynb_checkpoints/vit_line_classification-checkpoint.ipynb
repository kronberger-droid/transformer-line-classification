{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line-by-Line Classification with Vision Transformer\n",
    "\n",
    "A clean implementation of sequential scanline classification using a Vision Transformer (ViT) architecture.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook trains a transformer model to classify STM (Scanning Tunneling Microscope) images **while they're still being scanned**. Instead of waiting for the full image, the model can make predictions after seeing just a few scanlines.\n",
    "\n",
    "**Real-world use case**: Stop bad scans early to save time!\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Our approach treats each scanline as a single token:\n",
    "- **Input**: Partial scan with j scanlines (where j = 1, 2, ..., 128)\n",
    "- **Architecture**: Vision Transformer that treats each full line as one token\n",
    "- **Output**: Classification prediction based on lines seen so far\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Line Embedding**: Project each 128-pixel scanline to a 256-d embedding\n",
    "2. **Positional Encoding**: Tell the model which line comes first, second, etc.\n",
    "3. **Transformer Encoder**: Let all lines \"talk to each other\" via self-attention\n",
    "4. **Classification Head**: Predict the class from the CLS token\n",
    "\n",
    "## What to Expect\n",
    "\n",
    "- **Setup**: ~10 seconds\n",
    "- **Data loading**: ~5 seconds  \n",
    "- **Model creation**: Instant\n",
    "- **Training (2 epochs)**: ~5-10 minutes on CPU, ~30 seconds on GPU\n",
    "- **Full training (50 epochs)**: ~2-3 hours on CPU, ~20 minutes on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "**What this does**: Import libraries and configure PyTorch\n",
    "\n",
    "**Key libraries**:\n",
    "- `torch`: The deep learning framework we use\n",
    "- `nn` (neural network): Building blocks for our model\n",
    "- `Dataset/DataLoader`: Handle batching and feeding data to the model\n",
    "- `sklearn.metrics`: Evaluate model performance (accuracy, AUROC, etc.)\n",
    "- `tqdm`: Show progress bars during training\n",
    "\n",
    "**What to look for**: After running, check if you're using GPU or CPU. GPU is ~10-20x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "**What this section does**: Load and prepare STM images for training\n",
    "\n",
    "**Dataset format**:\n",
    "- Each image: [128 scanlines, 128 pixels per line]  \n",
    "- Each scanline: One row of the STM scan\n",
    "- Labels: Integer class labels (0, 1, 2, 3, 4, 5)\n",
    "\n",
    "**The key trick**: We create multiple training samples per image by randomly selecting different numbers of scanlines (e.g., 10 lines, 50 lines, 100 lines). This teaches the model to work with partial scans.\n",
    "\n",
    "**Why normalize each line?**: STM data has intensity variations due to instrumental drift. By normalizing each line independently (zero mean, unit variance), we help the model focus on patterns rather than absolute intensity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for line-by-line classification.\n",
    "    \n",
    "    For each image, we create multiple samples by varying the number of visible lines.\n",
    "    This simulates the real-time scanning scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, min_lines=5, max_lines=128, samples_per_image=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Array of shape [N, max_lines, line_width]\n",
    "            labels: Array of shape [N] with class labels\n",
    "            min_lines: Minimum number of lines to use\n",
    "            max_lines: Maximum number of lines (full image)\n",
    "            samples_per_image: How many random cutoffs to generate per image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.min_lines = min_lines\n",
    "        self.max_lines = max_lines\n",
    "        self.samples_per_image = samples_per_image\n",
    "        \n",
    "        # Normalize each scanline independently\n",
    "        self._normalize_scanlines()\n",
    "        \n",
    "    def _normalize_scanlines(self):\n",
    "        \"\"\"Normalize each scanline to zero mean and unit variance.\"\"\"\n",
    "        for i in range(len(self.images)):\n",
    "            for j in range(self.images.shape[1]):\n",
    "                line = self.images[i, j, :]\n",
    "                mean = line.mean()\n",
    "                std = line.std()\n",
    "                if std > 1e-8:\n",
    "                    self.images[i, j, :] = (line - mean) / std\n",
    "                else:\n",
    "                    self.images[i, j, :] = line - mean\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images) * self.samples_per_image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which image and which random cutoff\n",
    "        img_idx = idx // self.samples_per_image\n",
    "        \n",
    "        # Random number of lines between min and max\n",
    "        num_lines = np.random.randint(self.min_lines, self.max_lines + 1)\n",
    "        \n",
    "        # Get partial image\n",
    "        partial_image = self.images[img_idx, :num_lines, :]\n",
    "        label = self.labels[img_idx]\n",
    "        \n",
    "        return {\n",
    "            'image': torch.FloatTensor(partial_image),\n",
    "            'label': torch.LongTensor([label])[0],\n",
    "            'num_lines': num_lines\n",
    "        }\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"\n",
    "    Read dataset for testing\n",
    "    \"\"\"\n",
    "    data = np.load('data/processed_data.npz')\n",
    "\n",
    "    images = data[data.files[0]]\n",
    "    labels = data[data.files[1]]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from processed_data.npz...\n",
      "Total dataset: 18932 images\n",
      "Image size: 128 x 128\n",
      "Number of classes: 6\n",
      "Class distribution:\n",
      "  Class 0: 1664 samples (8.79%)\n",
      "  Class 1: 1482 samples (7.83%)\n",
      "  Class 2: 6830 samples (36.08%)\n",
      "  Class 3: 3202 samples (16.91%)\n",
      "  Class 4: 1332 samples (7.04%)\n",
      "  Class 5: 4422 samples (23.36%)\n",
      "\n",
      "Train set: 13259 images\n",
      "Val set: 2833 images\n",
      "Test set: 2840 images\n"
     ]
    }
   ],
   "source": [
    "# Load the real dataset\n",
    "print(\"Loading dataset from processed_data.npz...\")\n",
    "data = np.load('data/processed_data.npz')\n",
    "all_images = data[data.files[0]]\n",
    "all_labels = data[data.files[1]]\n",
    "\n",
    "print(f\"Total dataset: {all_images.shape[0]} images\")\n",
    "print(f\"Image size: {all_images.shape[1]} x {all_images.shape[2]}\")\n",
    "print(f\"Number of classes: {len(np.unique(all_labels))}\")\n",
    "print(f\"Class distribution:\")\n",
    "for cls in np.unique(all_labels):\n",
    "    count = np.sum(all_labels == cls)\n",
    "    print(f\"  Class {cls}: {count} samples ({100*count/len(all_labels):.2f}%)\")\n",
    "\n",
    "# Split into train/val/test (70/15/15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate test set\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.15, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "# Second split: separate validation set\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_val_images, train_val_labels, test_size=0.176, random_state=42, stratify=train_val_labels\n",
    ")  # 0.176 * 0.85 ≈ 0.15 of total\n",
    "\n",
    "print(f\"\\nTrain set: {train_images.shape[0]} images\")\n",
    "print(f\"Val set: {val_images.shape[0]} images\")\n",
    "print(f\"Test set: {test_images.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "**What this section does**: Build the transformer model piece by piece\n",
    "\n",
    "The transformer has 3 main components:\n",
    "1. **Line Embedding**: Convert raw pixel values to learnable representations\n",
    "2. **Positional Encoding**: Add position information (which line is which)\n",
    "3. **Transformer Layers**: Process the sequence with self-attention\n",
    "\n",
    "**Why transformers?** They excel at sequence data because they can:\n",
    "- Look at ALL scanlines simultaneously (not just nearby ones)\n",
    "- Learn which parts of the sequence are important via attention\n",
    "- Handle variable-length inputs (10 lines or 100 lines - doesn't matter!)\n",
    "\n",
    "### 3.1 Line Embedding\n",
    "\n",
    "**What it does**: Projects each 128-pixel scanline to a 256-dimensional embedding\n",
    "\n",
    "Think of it like compression: 128 raw numbers → 256 \"feature\" numbers that capture the essence of the line.\n",
    "\n",
    "**Why 256 dimensions?** This is a hyperparameter. Larger = more capacity but slower training.\n",
    "Common values: 128, 256, 512, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert scanlines into embeddings for the transformer.\n",
    "    \n",
    "    This module treats each full scanline as a single token and projects it\n",
    "    into a higher-dimensional embedding space. This is more natural for 1D\n",
    "    sequential data than subdividing lines into patches.\n",
    "    \n",
    "    For example, with line_width=128:\n",
    "    - Each full 128-pixel scanline becomes one token\n",
    "    - Each line is then projected to embed_dim dimensions\n",
    "    - This results in num_lines tokens (much more efficient than patch-based approach)\n",
    "    \n",
    "    Args:\n",
    "        line_width: Number of pixels in each scanline (default: 128)\n",
    "        embed_dim: Dimensionality of the embedding space (default: 256)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, line_width=128, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.line_width = line_width\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projection from full line to embed_dim\n",
    "        self.projection = nn.Linear(line_width, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Convert scanlines to embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, num_lines, line_width]\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: [batch_size, num_lines, embed_dim]\n",
    "            where each line is one token\n",
    "        \"\"\"\n",
    "        batch_size, num_lines, line_width = x.shape\n",
    "        \n",
    "        # Project each line directly to embedding dimension\n",
    "        # Shape: (batch_size, num_lines, line_width) -> (batch_size, num_lines, embed_dim)\n",
    "        embeddings = self.projection(x)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Positional Encoding\n",
    "\n",
    "**What it does**: Tells the model \"this is line 1, this is line 2, etc.\"\n",
    "\n",
    "**Why needed?** Transformers process all tokens in parallel. Without positional encoding, the model can't tell if it's seeing lines in order [1,2,3] or scrambled [3,1,2].\n",
    "\n",
    "**How it works**: We use sinusoidal functions (sin/cos) at different frequencies:\n",
    "- Low frequencies: Capture \"rough\" position (early vs late in scan)\n",
    "- High frequencies: Capture \"precise\" position (line 42 vs line 43)\n",
    "\n",
    "**Benefits of sinusoidal encoding**:\n",
    "- Works for any sequence length (even longer than training!)\n",
    "- Model can learn to attend to relative positions (\"5 lines ago\")\n",
    "- No additional parameters to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Add positional information to line embeddings using sinusoidal encoding.\n",
    "    \n",
    "    Transformers have no inherent notion of sequence order, so we add positional\n",
    "    encodings to give the model information about the position of each scanline.\n",
    "    This is crucial for STM data where the temporal order of scanlines matters.\n",
    "    \n",
    "    We use the sinusoidal encoding from \"Attention is All You Need\" (Vaswani et al.):\n",
    "    - PE(pos, 2i) = sin(pos / 10000^(2i/embed_dim))\n",
    "    - PE(pos, 2i+1) = cos(pos / 10000^(2i/embed_dim))\n",
    "    \n",
    "    This encoding allows the model to learn relative positions and generalizes\n",
    "    well to sequence lengths not seen during training.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimensionality of the embeddings\n",
    "        max_lines: Maximum number of scanlines expected\n",
    "        dropout: Dropout rate applied after adding positional encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_lines=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Maximum sequence length: max_lines + 1 (for CLS token)\n",
    "        max_len = max_lines + 1\n",
    "        \n",
    "        # Initialize positional encoding matrix\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calculate the division term for sinusoidal functions\n",
    "        # This creates different frequencies for different dimensions\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        \n",
    "        # Apply sine to even dimensions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be moved to GPU)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Vision Transformer (The Complete Model)\n",
    "\n",
    "**What it does**: Combines everything into a complete classification model\n",
    "\n",
    "**Architecture flow**:\n",
    "```\n",
    "Input: [batch, num_lines, 128]\n",
    "  ↓ Line Embedding\n",
    "[batch, num_lines, 256]\n",
    "  ↓ Add CLS token\n",
    "[batch, 1+num_lines, 256]  ← CLS token prepended\n",
    "  ↓ Add Positional Encoding\n",
    "[batch, 1+num_lines, 256]\n",
    "  ↓ Transformer Layers (6 layers)\n",
    "[batch, 1+num_lines, 256]\n",
    "  ↓ Extract CLS token\n",
    "[batch, 256]\n",
    "  ↓ Classification Head\n",
    "[batch, 6]  ← Logits for 6 classes\n",
    "```\n",
    "\n",
    "**Key design choices**:\n",
    "- **CLS token**: A learnable \"summary\" token that aggregates information from all lines\n",
    "- **6 transformer layers**: Each layer refines the representations using self-attention\n",
    "- **8 attention heads**: Each head can focus on different patterns\n",
    "- **Pre-normalization**: Normalizes before (not after) each sub-layer for better training stability\n",
    "\n",
    "**What happens during training**: The model learns to:\n",
    "1. Embed lines in a meaningful way (similar lines → similar embeddings)\n",
    "2. Attend to relevant parts of the sequence (which lines matter for classification?)\n",
    "3. Aggregate information into the CLS token\n",
    "4. Map CLS token to class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer adapted for line-by-line STM image classification.\n",
    "    \n",
    "    This model follows the ViT architecture but treats each full scanline as a token:\n",
    "    1. Line Embedding: Convert full scanlines to embeddings (one token per line)\n",
    "    2. CLS Token: Add a learnable classification token at the start of sequence\n",
    "    3. Positional Encoding: Add position information to embeddings\n",
    "    4. Transformer Encoder: Process the sequence with self-attention\n",
    "    5. Classification Head: Use the CLS token output for classification\n",
    "    \n",
    "    The model can handle variable-length inputs (different numbers of scanlines)\n",
    "    thanks to the CLS token approach and positional encodings.\n",
    "    \n",
    "    This architecture is more efficient than patch-based approaches:\n",
    "    - num_lines tokens instead of num_lines × patches_per_line tokens\n",
    "    - Quadratically reduced attention computation\n",
    "    - More natural for 1D sequential data\n",
    "    \n",
    "    Args:\n",
    "        line_width: Number of pixels per scanline (default: 128)\n",
    "        num_classes: Number of output classes (default: 6)\n",
    "        embed_dim: Embedding dimension (default: 256)\n",
    "        depth: Number of transformer encoder layers (default: 6)\n",
    "        num_heads: Number of attention heads (default: 8)\n",
    "        mlp_ratio: Ratio of MLP hidden dim to embedding dim (default: 4)\n",
    "        dropout: Dropout rate (default: 0.1)\n",
    "        max_lines: Maximum number of scanlines (default: 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        line_width=128,\n",
    "        num_classes=4,\n",
    "        embed_dim=256,\n",
    "        depth=6,\n",
    "        num_heads=8,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1,\n",
    "        max_lines=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.line_width = line_width\n",
    "        \n",
    "        # Convert scanlines to embeddings (one token per line)\n",
    "        self.patch_embed = PatchEmbedding(line_width, embed_dim)\n",
    "        \n",
    "        # Learnable classification token (similar to BERT's [CLS] token)\n",
    "        # This token's output will be used for classification\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Add positional information to embeddings\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            embed_dim, \n",
    "            max_lines=max_lines, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * mlp_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm architecture\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights following ViT paper.\"\"\"\n",
    "        # Initialize CLS token\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Vision Transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch_size, num_lines, line_width]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Convert scanlines to line embeddings\n",
    "        # Shape: [batch_size, num_lines, embed_dim]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Prepend CLS token: [batch, num_lines + 1, embed_dim]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Take CLS token output\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "        # Normalize and classify\n",
    "        cls_output = self.norm(cls_output)\n",
    "        logits = self.head(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 4,773,894 trainable parameters\n",
      "Input shape: torch.Size([2, 50, 128])\n",
      "Output shape: torch.Size([2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/1sr1qfyy3i4jwlsy7bsk23pgf838ms3q-python3-3.13.8-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = VisionTransformer(\n",
    "    line_width=128,\n",
    "    num_classes=6,  # Updated to 6 classes based on the dataset\n",
    "    embed_dim=256,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1,\n",
    "    max_lines=128\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model created with {num_params:,} trainable parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 50, 128).to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "**What this section does**: Configure how the model learns\n",
    "\n",
    "This is where you define:\n",
    "1. **How to measure error** (loss function)\n",
    "2. **How to update weights** (optimizer)\n",
    "3. **How to adjust learning rate** (scheduler)\n",
    "4. **How to batch data** (dataloaders)\n",
    "\n",
    "### Key Hyperparameters (feel free to experiment!)\n",
    "\n",
    "**samples_per_image**: How many random line-count variations per image\n",
    "- Current: 2 (fast prototyping)\n",
    "- For real training: 5-10\n",
    "- Higher = more data augmentation, longer training\n",
    "\n",
    "**batch_size**: How many samples to process before updating weights\n",
    "- Current: 8 (uses less memory)\n",
    "- Typical: 16-32\n",
    "- Larger = more stable gradients, faster training, more memory\n",
    "\n",
    "**Learning rate (lr)**: How big are weight updates?\n",
    "- Default: 1e-4 (0.0001)\n",
    "- Too high: Training diverges\n",
    "- Too low: Training is very slow\n",
    "- This is THE most important hyperparameter!\n",
    "\n",
    "**num_epochs**: How many times to see the full dataset\n",
    "- Current: 2 (quick test)\n",
    "- For real training: 50-100\n",
    "- More epochs = better performance (until overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 3315\n",
      "Validation batches: 709\n",
      "Test batches: 710\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    Pads to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    max_lines = max(item['num_lines'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    line_width = batch[0]['image'].shape[1]\n",
    "    \n",
    "    # Create padded tensors\n",
    "    images = torch.zeros(batch_size, max_lines, line_width)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        num_lines = item['num_lines']\n",
    "        images[i, :num_lines] = item['image']\n",
    "        labels[i] = item['label']\n",
    "    \n",
    "    return {'images': images, 'labels': labels}\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "# For real training: samples_per_image=5\n",
    "train_dataset = LineByLineDataset(train_images, train_labels, min_lines=10, samples_per_image=2)\n",
    "val_dataset = LineByLineDataset(val_images, val_labels, min_lines=10, samples_per_image=2)\n",
    "test_dataset = LineByLineDataset(test_images, test_labels, min_lines=10, samples_per_image=2)\n",
    "\n",
    "# Create dataloaders\n",
    "# For real training: batch_size=32\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "- Loss function: CrossEntropyLoss (standard for classification)\n",
      "- Optimizer: AdamW (Adam with better weight decay)\n",
      "- Learning rate: 1e-4 (0.0001)\n",
      "- Weight decay: 0.05 (L2 regularization to prevent overfitting)\n",
      "- LR Scheduler: CosineAnnealing (gradually reduces LR)\n",
      "- Epochs: 2\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "\n",
    "# Learning rate scheduler\n",
    "# For real training: num_epochs=50\n",
    "num_epochs = 2\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Loss function: CrossEntropyLoss (standard for classification)\")\n",
    "print(f\"- Optimizer: AdamW (Adam with better weight decay)\")\n",
    "print(f\"- Learning rate: 1e-4 (0.0001)\")\n",
    "print(f\"- Weight decay: 0.05 (L2 regularization to prevent overfitting)\")\n",
    "print(f\"- LR Scheduler: CosineAnnealing (gradually reduces LR)\")\n",
    "print(f\"- Epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "**What this section does**: Actually train the model!\n",
    "\n",
    "### The Training Process (what happens each epoch):\n",
    "\n",
    "1. **Training phase**: \n",
    "   - Process all training batches\n",
    "   - For each batch: forward pass → compute loss → backward pass → update weights\n",
    "   - Track loss and accuracy\n",
    "   \n",
    "2. **Validation phase**:\n",
    "   - Process all validation batches (no weight updates!)\n",
    "   - Measure performance on unseen data\n",
    "   - Track loss, accuracy, and AUROC\n",
    "\n",
    "3. **Save best model**:\n",
    "   - If validation AUROC improves, save checkpoint\n",
    "   - This prevents overfitting (use model from best epoch, not last epoch)\n",
    "\n",
    "### What to Watch For:\n",
    "\n",
    "**Good signs**:\n",
    "- Training loss decreases steadily\n",
    "- Validation loss decreases (or stays stable)\n",
    "- Validation accuracy improves\n",
    "- Gap between train/val is small\n",
    "\n",
    "**Bad signs**:\n",
    "- Training loss decreases but validation loss increases = **overfitting**\n",
    "- Both losses stay high = **underfitting** (need bigger model or more epochs)\n",
    "- Loss becomes NaN = learning rate too high or numerical instability\n",
    "\n",
    "**Metrics explained**:\n",
    "- **Loss**: How wrong the predictions are (lower = better)\n",
    "- **Accuracy**: % of correct predictions (higher = better)\n",
    "- **Balanced Accuracy**: Like accuracy but accounts for class imbalance\n",
    "- **AUROC**: Area under ROC curve (0.5 = random, 1.0 = perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct / total})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            images = batch['images'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
    "    auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return total_loss / len(loader), accuracy, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Watch the progress bars and metrics below.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/2\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5d3aa478ac4918b64a30f110d1b432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_auroc': []\n",
    "}\n",
    "\n",
    "best_val_auroc = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_auroc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auroc'].append(val_auroc)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUROC: {val_auroc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_auroc': val_auroc,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"Saved best model with AUROC: {val_auroc:.4f}\")\n",
    "    else:\n",
    "        print(f\" No improvement. Best AUROC: {best_val_auroc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation AUROC: {best_val_auroc:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "**What this does**: Plot training curves to visualize learning progress\n",
    "\n",
    "**How to interpret the plots**:\n",
    "\n",
    "1. **Loss plot** (left):\n",
    "   - Should decrease over time for both train and val\n",
    "   - If val loss increases while train decreases: **overfitting**\n",
    "   - If both stay flat: **underfitting** (need bigger model or more epochs)\n",
    "\n",
    "2. **Accuracy plot** (middle):\n",
    "   - Should increase over time\n",
    "   - Balanced accuracy accounts for class imbalance (better than regular accuracy)\n",
    "   - Gap between train/val shows generalization quality\n",
    "\n",
    "3. **AUROC plot** (right):\n",
    "   - Most important metric for imbalanced classification\n",
    "   - 0.5 = random guessing\n",
    "   - 1.0 = perfect classification\n",
    "   - For 6-class problem, 0.8+ is good, 0.9+ is excellent\n",
    "\n",
    "**What good training looks like**:\n",
    "- Smooth curves (not too noisy)\n",
    "- Train and val curves close together\n",
    "- Steady improvement over time\n",
    "- Val metrics plateau towards the end (learning saturated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Balanced Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUROC\n",
    "axes[2].plot(history['val_auroc'], label='Validation', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUROC')\n",
    "axes[2].set_title('Validation AUROC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Performance vs. Number of Lines\n",
    "\n",
    "**This is the key analysis!** \n",
    "\n",
    "We want to answer: \"How many scanlines do we need to classify accurately?\"\n",
    "\n",
    "**What this does**:\n",
    "- Evaluates the model with 10, 20, 30, ..., 128 lines\n",
    "- Shows how performance improves as we see more of the image\n",
    "- Helps decide: \"When can we stop scanning early?\"\n",
    "\n",
    "**What to expect**:\n",
    "- Performance improves as we see more lines (more information)\n",
    "- Early lines might give ~60-70% accuracy\n",
    "- Full image (128 lines) might give ~90%+ accuracy\n",
    "- The curve usually shows diminishing returns (plateaus after some point)\n",
    "\n",
    "**Practical implications**:\n",
    "- If 50 lines gives 85% accuracy and 128 lines gives 88%, maybe stop at 50?\n",
    "- Saves 60% of scan time with minimal accuracy loss!\n",
    "- This is the value proposition of early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_num_lines(model, images, labels, device, line_steps=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance as a function of number of scanlines.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        images: Full images [N, max_lines, line_width]\n",
    "        labels: Labels [N]\n",
    "        device: Device to run on\n",
    "        line_steps: List of line counts to evaluate at (default: [10, 20, 30, ..., 128])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics at each line count\n",
    "    \"\"\"\n",
    "    if line_steps is None:\n",
    "        line_steps = list(range(10, 129, 10))  # Every 10 lines\n",
    "    \n",
    "    model.eval()\n",
    "    results = {'num_lines': [], 'accuracy': [], 'auroc': []}\n",
    "    \n",
    "    for num_lines in tqdm(line_steps, desc='Evaluating vs. num lines'):\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        all_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), 32):  # Batch size 32\n",
    "                batch_images = images[i:i+32, :num_lines, :]\n",
    "                batch_labels = labels[i:i+32]\n",
    "                \n",
    "                batch_images = torch.FloatTensor(batch_images).to(device)\n",
    "                \n",
    "                logits = model(batch_images)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_labels.extend(batch_labels)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "        all_preds = np.array(all_preds)\n",
    "        \n",
    "        accuracy = balanced_accuracy_score(all_labels, all_preds)\n",
    "        auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
    "        \n",
    "        results['num_lines'].append(num_lines)\n",
    "        results['accuracy'].append(accuracy)\n",
    "        results['auroc'].append(auroc)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate on test set\n",
    "results = evaluate_vs_num_lines(model, test_images, test_labels, device)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUROC vs. number of lines\n",
    "ax1.plot(results['num_lines'], results['auroc'], marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Number of Scanlines')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax1.set_title('AUROC vs. Number of Scanlines')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Accuracy vs. number of lines\n",
    "ax2.plot(results['num_lines'], results['accuracy'], marker='o', linewidth=2, markersize=6, color='green')\n",
    "ax2.set_xlabel('Number of Scanlines')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('Accuracy vs. Number of Scanlines')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_vs_lines.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for i, num_lines in enumerate(results['num_lines']):\n",
    "    print(f\"Lines: {num_lines:3d} | AUROC: {results['auroc'][i]:.4f} | Accuracy: {results['accuracy'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix\n",
    "\n",
    "**What this does**: Shows which classes the model confuses\n",
    "\n",
    "**How to read it**:\n",
    "- Rows = True labels (what the class actually is)\n",
    "- Columns = Predicted labels (what the model thinks)\n",
    "- Diagonal = Correct predictions\n",
    "- Off-diagonal = Mistakes\n",
    "\n",
    "**What to look for**:\n",
    "- Strong diagonal = good performance\n",
    "- Large off-diagonal numbers = systematic confusion\n",
    "- Example: If row 2, column 3 is large → model often predicts class 3 when truth is class 2\n",
    "\n",
    "**Why this matters**:\n",
    "- Helps debug: \"Why is class X performing poorly?\"\n",
    "- May reveal that classes are inherently similar\n",
    "- Can guide data collection: \"Need more examples of confused classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on full test set\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Class {i}' for i in range(6)],\n",
    "            yticklabels=[f'Class {i}' for i in range(6)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Visualization (Optional)\n",
    "\n",
    "TODO: Extract and visualize attention maps to understand what the model focuses on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify model to return attention weights\n",
    "# TODO: Visualize which patches/lines the model attends to\n",
    "# This helps interpret model decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export\n",
    "\n",
    "Export model for deployment or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model architecture and weights\n",
    "import json\n",
    "\n",
    "model_config = {\n",
    "    'line_width': 128,\n",
    "    'num_classes': 6,  # Updated to 6 classes\n",
    "    'embed_dim': 256,\n",
    "    'depth': 6,\n",
    "    'num_heads': 8,\n",
    "    'mlp_ratio': 4,\n",
    "    'dropout': 0.1,\n",
    "    'max_lines': 128\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_config': model_config,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_val_auroc': best_val_auroc\n",
    "}, 'final_model.pt')\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Best validation AUROC: {best_val_auroc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
